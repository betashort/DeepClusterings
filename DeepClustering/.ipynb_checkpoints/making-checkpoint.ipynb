{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Clustering for Unsupervised Learning of Visual Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alexnet import AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==== random seeds ====\n",
    "#乱数の設定\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "#==== CNN model ====\n",
    "#モデルの定義\n",
    "\n",
    "#==== Clustering Algorithm ====\n",
    "#クラスタリング　アルゴリズムの定義\n",
    "#k-meansなど\n",
    "\n",
    "\n",
    "\n",
    "#==== optimizer ====\n",
    "#最適化の定義\n",
    "optimizer = torch.optim.SGD(\n",
    "    filter(lambda x: x.requires_grad, model.parameters()),\n",
    "    lr=args.lr,\n",
    "    momentum=args.momentum,\n",
    "    weight_decay=10**args.wd,\n",
    ")\n",
    "\n",
    "#==== define loss function ====\n",
    "#損失関数の定義\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net  = AlexNet(1, 3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor(10, 1, 224, 224)\n",
    "x = x.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.to('cpu').detach().clone().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.utils.data.DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loader, ):\n",
    "    #==== args ====\n",
    "    #model: CNN etc...\n",
    "    #loader: dataset\n",
    "    #\n",
    "    \n",
    "    # define loss function\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    \n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        #==== All data ====\n",
    "        #clustering for create psuedo_label with all data\n",
    "        \n",
    "        # forward\n",
    "        features = model.forward()\n",
    "        features = features.to('cpu').detach().clone().numpy()\n",
    "        \n",
    "        # clustering\n",
    "        clustering = Kmeans(n_cluster=3)\n",
    "        psuedo_label = clunstering.fit_predict(features)\n",
    "        \n",
    "        #==== train by pseudo-labels use batch ====\n",
    "        #train_dataloader\n",
    "        train_loader = (X, psuedo_lable, 'train')\n",
    "    \n",
    "        for i, (image, label) in enumerate(loader):\n",
    "            image = image.to(device)\n",
    "            label = image.to(device)\n",
    "            \n",
    "            output = net(image)\n",
    "            loss = criterion(output, label)\n",
    "            \n",
    "            loss.update\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Create_Datasets(Dataset):\n",
    "     \n",
    "    def __init__(self, image_lists, labels, data_transform=None, train=True):\n",
    "        assert len(image_lists) == len(labels), 'image != label'\n",
    "        \n",
    "        self.image_lists = image_lists\n",
    "        self.labels = labels\n",
    "        self.data_transform = data_transform\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_lists)\n",
    "     \n",
    "    def __getitem__(self, idx):\n",
    "        file = fa\n",
    "        image = fa\n",
    "        label = labels[idx]\n",
    "        \n",
    " \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "image != label",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-a2649f7fe9bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mCreate_Datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-42-75035916e491>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, image_lists, labels, data_transform, train)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_lists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_lists\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'image != label'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_lists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: image != label"
     ]
    }
   ],
   "source": [
    "Create_Datasets([1,1], [3,4, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    # fix random seeds\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "\n",
    "    # CNN\n",
    "    if args.verbose:\n",
    "        print('Architecture: {}'.format(args.arch))\n",
    "    model = models.__dict__[args.arch](sobel=args.sobel)\n",
    "    fd = int(model.top_layer.weight.size()[1])\n",
    "    model.top_layer = None\n",
    "    model.features = torch.nn.DataParallel(model.features)\n",
    "    model.cuda()\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    \n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
    "            checkpoint = torch.load(args.resume)\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            # remove top_layer parameters from checkpoint\n",
    "            for key in checkpoint['state_dict']:\n",
    "                if 'top_layer' in key:\n",
    "                    del checkpoint['state_dict'][key]\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(args.resume, checkpoint['epoch']))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
    "\n",
    "    # creating checkpoint repo\n",
    "    exp_check = os.path.join(args.exp, 'checkpoints')\n",
    "    if not os.path.isdir(exp_check):\n",
    "        os.makedirs(exp_check)\n",
    "\n",
    "    # creating cluster assignments log\n",
    "    cluster_log = Logger(os.path.join(args.exp, 'clusters'))\n",
    "\n",
    "    # preprocessing of data\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    tra = [transforms.Resize(256),\n",
    "           transforms.CenterCrop(224),\n",
    "           transforms.ToTensor(),\n",
    "           normalize]\n",
    "\n",
    "    # load the data\n",
    "    end = time.time()\n",
    "    dataset = datasets.ImageFolder(args.data, transform=transforms.Compose(tra))\n",
    "    if args.verbose:\n",
    "        print('Load dataset: {0:.2f} s'.format(time.time() - end))\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                             batch_size=args.batch,\n",
    "                                             num_workers=args.workers,\n",
    "                                             pin_memory=True)\n",
    "\n",
    "    # clustering algorithm to use\n",
    "    deepcluster = clustering.__dict__[args.clustering](args.nmb_cluster)\n",
    "\n",
    "    # training convnet with DeepCluster\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        end = time.time()\n",
    "\n",
    "        # remove head\n",
    "        model.top_layer = None\n",
    "        model.classifier = nn.Sequential(*list(model.classifier.children())[:-1])\n",
    "\n",
    "        # get the features for the whole dataset\n",
    "        features = compute_features(dataloader, model, len(dataset))\n",
    "\n",
    "        # cluster the features\n",
    "        if args.verbose:\n",
    "            print('Cluster the features')\n",
    "        clustering_loss = deepcluster.cluster(features, verbose=args.verbose)\n",
    "\n",
    "        # assign pseudo-labels\n",
    "        if args.verbose:\n",
    "            print('Assign pseudo labels')\n",
    "        train_dataset = clustering.cluster_assign(deepcluster.images_lists,\n",
    "                                                  dataset.imgs)\n",
    "\n",
    "        # uniformly sample per target\n",
    "        sampler = UnifLabelSampler(int(args.reassign * len(train_dataset)),\n",
    "                                   deepcluster.images_lists)\n",
    "\n",
    "        train_dataloader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=args.batch,\n",
    "            num_workers=args.workers,\n",
    "            sampler=sampler,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "        # set last fully connected layer\n",
    "        mlp = list(model.classifier.children())\n",
    "        mlp.append(nn.ReLU(inplace=True).cuda())\n",
    "        model.classifier = nn.Sequential(*mlp)\n",
    "        model.top_layer = nn.Linear(fd, len(deepcluster.images_lists))\n",
    "        model.top_layer.weight.data.normal_(0, 0.01)\n",
    "        model.top_layer.bias.data.zero_()\n",
    "        model.top_layer.cuda()\n",
    "\n",
    "        # train network with clusters as pseudo-labels\n",
    "        end = time.time()\n",
    "        loss = train(train_dataloader, model, criterion, optimizer, epoch)\n",
    "\n",
    "        # print log\n",
    "        if args.verbose:\n",
    "            print('###### Epoch [{0}] ###### \\n'\n",
    "                  'Time: {1:.3f} s\\n'\n",
    "                  'Clustering loss: {2:.3f} \\n'\n",
    "                  'ConvNet loss: {3:.3f}'\n",
    "                  .format(epoch, time.time() - end, clustering_loss, loss))\n",
    "            try:\n",
    "                nmi = normalized_mutual_info_score(\n",
    "                    clustering.arrange_clustering(deepcluster.images_lists),\n",
    "                    clustering.arrange_clustering(cluster_log.data[-1])\n",
    "                )\n",
    "                print('NMI against previous assignment: {0:.3f}'.format(nmi))\n",
    "            except IndexError:\n",
    "                pass\n",
    "            print('####################### \\n')\n",
    "        # save running checkpoint\n",
    "        torch.save({'epoch': epoch + 1,\n",
    "                    'arch': args.arch,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'optimizer' : optimizer.state_dict()},\n",
    "                   os.path.join(args.exp, 'checkpoint.pth.tar'))\n",
    "\n",
    "        # save cluster assignments\n",
    "        cluster_log.log(deepcluster.images_lists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_assign(images_lists, dataset):\n",
    "    \"\"\"Creates a dataset from clustering, with clusters as labels.\n",
    "    Args:\n",
    "        images_lists (list of list): for each cluster, the list of image indexes\n",
    "                                    belonging to this cluster\n",
    "        dataset (list): initial dataset\n",
    "    Returns:\n",
    "        ReassignedDataset(torch.utils.data.Dataset): a dataset with clusters as\n",
    "                                                     labels\n",
    "    \"\"\"\n",
    "    assert images_lists is not None\n",
    "    pseudolabels = []\n",
    "    image_indexes = []\n",
    "    for cluster, images in enumerate(images_lists):\n",
    "        image_indexes.extend(images)\n",
    "        pseudolabels.extend([cluster] * len(images))\n",
    "\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    t = transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                            transforms.RandomHorizontalFlip(),\n",
    "                            transforms.ToTensor(),\n",
    "                            normalize])\n",
    "\n",
    "    return ReassignedDataset(image_indexes, pseudolabels, dataset, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReassignedDataset(data.Dataset):\n",
    "    \"\"\"A dataset where the new images labels are given in argument.\n",
    "    Args:\n",
    "        image_indexes (list): list of data indexes\n",
    "        pseudolabels (list): list of labels for each data\n",
    "        dataset (list): list of tuples with paths to images\n",
    "        transform (callable, optional): a function/transform that takes in\n",
    "                                        an PIL image and returns a\n",
    "                                        transformed version\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_indexes, pseudolabels, dataset, transform=None):\n",
    "        self.imgs = self.make_dataset(image_indexes, pseudolabels, dataset)\n",
    "        self.transform = transform\n",
    "\n",
    "    def make_dataset(self, image_indexes, pseudolabels, dataset):\n",
    "        label_to_idx = {label: idx for idx, label in enumerate(set(pseudolabels))}\n",
    "        images = []\n",
    "        for j, idx in enumerate(image_indexes):\n",
    "            path = dataset[idx][0]\n",
    "            pseudolabel = label_to_idx[pseudolabels[j]]\n",
    "            images.append((path, pseudolabel))\n",
    "        return images\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): index of data\n",
    "        Returns:\n",
    "            tuple: (image, pseudolabel) where pseudolabel is the cluster of index datapoint\n",
    "        \"\"\"\n",
    "        path, pseudolabel = self.imgs[index]\n",
    "        img = pil_loader(path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, pseudolabel\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
